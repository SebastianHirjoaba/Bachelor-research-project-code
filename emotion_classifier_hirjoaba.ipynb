{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be23493",
   "metadata": {},
   "source": [
    "### How to run this notebook\n",
    "\n",
    "Note: The folders with the airlinetweets and MELD data files, as given in the course, need to be placed in the same directory with this notebook.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [1. Install and import](#section1)\n",
    "* [2. Inspect the datasets](#section2)\n",
    "   * [2.2 MELD](#section2.2)\n",
    "* [3. Define the functions](#section3)\n",
    "   * [3.1 load the data](#section3.1)\n",
    "   * [3.2 convert the data to numerical representations](#section3.2)\n",
    "   * [3.3 train the classifier and get the predictions](#section3.3)\n",
    "   * [3.4 print the report](#section3.4)\n",
    "* [4. Run the functions](#section4)\n",
    "   * [4.2 Emotion analysis with MELD](#section4.2)\n",
    "* [5. Apply the trained classifiers](#section5)\n",
    "   * [5.1 Sentiment analysis trained with airli\n",
    "   * [5.2 Emotion analysis trained with MELD data](#section5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518edc4",
   "metadata": {},
   "source": [
    "## 1. Install and import <a class=\"anchor\" id =\"section1\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee59e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3360fc",
   "metadata": {},
   "source": [
    "## 2. Inspect the datasets <a class=\"anchor\" id =\"section2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3693ce",
   "metadata": {},
   "source": [
    "##### 2.1 airlinetweets <a class=\"anchor\" id =\"section2.1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2984fa",
   "metadata": {},
   "source": [
    "##### 2.2 MELD <a class=\"anchor\" id =\"section2.2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MELD\n",
    "# the textual data of MELD are provided in MELD folder.\n",
    "# The data is separated in three structured csv files: train_sent_emo.csv, dev_sent_emo.csv, test_sent_emo.csv \n",
    "# for this type of data format, Pandas is very powerful and handy for data loading\n",
    "# https://pandas.pydata.org\n",
    "# Path to the MELD data\n",
    "filepath_MELD = 'train_sent_emo.csv'# if you wanna observe other files, change 'train' to 'dev' or 'test'\n",
    "\n",
    "# Create a dataframe object 'df_' by readig the file.\n",
    "df_MELD = pd.read_csv(filepath_MELD)\n",
    "\n",
    "# to fix encoding problems and replace the 'Utterance' columns with the clean strings\n",
    "df_MELD['Utterance'] = df_MELD['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
    "\n",
    "# to print out the first cell in the 'Utterance' column to inspect it.\n",
    "print(df_MELD['Utterance'][0])\n",
    "print('----------------------------------------------------------------------------------------------')  \n",
    "# to print out the first 5 rows in the 'Utterance' column.\n",
    "print(df_MELD.head(5))\n",
    "print('----------------------------------------------------------------------------------------------')  \n",
    "# to print out the last 5 rows in the 'Utterance' column.\n",
    "print(df_MELD.tail(5))\n",
    "print('----------------------------------------------------------------------------------------------')  \n",
    "# to print out the last 5 rows in the 'Utterance' column without using print(), now much easier to read.\n",
    "df_MELD.tail(5)\n",
    "# to print out all emotion and sentiment values\n",
    "print(df_MELD['Emotion'].value_counts())\n",
    "print('----------------------------------------------------------------------------------------------')  \n",
    "# to print out all emotion and sentiment values\n",
    "print(df_MELD['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465c401",
   "metadata": {},
   "source": [
    "## 3. Define the functions <a class=\"anchor\" id =\"section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253abe9",
   "metadata": {},
   "source": [
    "##### 3.1 load the data <a class=\"anchor\" id =\"section3.1\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MELD data\n",
    "def load_MELD_data():\n",
    "        # Path to the training data\n",
    "    filepath_MELD_train = 'train_sent_emo.csv'\n",
    "\n",
    "    # Create a dataframe object 'dftrain' by readig the file.\n",
    "    dftrain = pd.read_csv(filepath_MELD_train)\n",
    "\n",
    "    ### The data has some problematic strings with encoding problems. The next code removes some of these from the utterances\n",
    "    # Fixing encoding problems and replacing the 'Utterance' columns with the cleaned strings\n",
    "    dftrain['Utterance'] = dftrain['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
    "    # Path to the test data\n",
    "    filepath_MELD_test = 'test_sent_emo.csv'\n",
    "    dftest = pd.read_csv(filepath_MELD_test)\n",
    "    dftest['Utterance'] = dftest['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
    "        #to prepare the vectorization, we need to collect the sentences and labels to lists.\n",
    "    #to prepare the training data\n",
    "    training_instances=[]\n",
    "    for utterance in dftrain['Utterance']:\n",
    "        ### If your computer has performance issue, you can break the loop after 2000 instances to have less data, see the following two lines.\n",
    "        #if index==2000:\n",
    "        #    break\n",
    "        training_instances.append(utterance)\n",
    "\n",
    "    ### print the length of our list to see if al data are loaded\n",
    "    print(len(training_instances))\n",
    "\n",
    "    training_labels = []\n",
    "    for label in dftrain['Emotion']:\n",
    "        ### the index need to be set the same as in the previous for-loop\n",
    "        #if index==2000:\n",
    "        #    break\n",
    "        training_labels.append(label)\n",
    "    ### Check if we have the same number of labels\n",
    "    print(len(training_labels))\n",
    "\n",
    "    # to prepare the test data\n",
    "    test_instances = []\n",
    "    for utterance in dftest['Utterance']:\n",
    "        test_instances.append(utterance)\n",
    "\n",
    "    ### We use the same loop for the list of emotion labels that correspond with the vector representations of each utterance\n",
    "    test_labels = []\n",
    "    for label in dftest['Emotion']:\n",
    "        test_labels.append(label)\n",
    "    target_labels= list(set(test_labels+training_labels))\n",
    "    \n",
    "    return training_instances,test_instances,training_labels,test_labels,target_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363be77",
   "metadata": {},
   "source": [
    "##### 3.2 convert the data to numerical representations <a class=\"anchor\" id =\"section3.2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d06cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert MELD data to numerical representation\n",
    "def data_MELD_to_nrrepr(training_instances,test_instances,training_labels,test_labels):\n",
    "\n",
    "        \n",
    "    # Turn (utterances) train data into a vector \n",
    "    frequency_threshold = 4\n",
    "    utterance_vec =CountVectorizer(min_df=frequency_threshold, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                                 tokenizer=nltk.word_tokenize # we use the nltk tokenizer\n",
    "                                 ) # stopwords are removed\n",
    "\n",
    "    training_count_vectors = utterance_vec.fit_transform(training_instances)\n",
    "    print(training_count_vectors.shape)\n",
    "\n",
    "    # Convert raw frequency counts into TF-IDF values\n",
    "    # why TF-IDF? when we transfer the utterance to vectors, the CountVectorizer treat all words equally, recall BOW (bag-of-words)\n",
    "    # the disadvantage is a more frequent word, such as 'and', may get more weight than a much more informative word.\n",
    "    # TF-IDF solves this issue by giving less weight to words of high frequency that occur in many documents.\n",
    "    # an extensive explanation https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/\n",
    "    #The shape remains the same but the values are now scores between zero and one.\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    training_tfidf_vectors = tfidf_transformer.fit_transform(training_count_vectors)\n",
    "\n",
    "    # Turn test data into a vector \n",
    "    test_count_vectors = utterance_vec.transform(test_instances)\n",
    "    test_tfidf_vectors = tfidf_transformer.fit_transform(test_count_vectors)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "# we feed this encoder with the complete list of labels from our data, both the training and test labels\n",
    "    label_encoder.fit(training_labels+test_labels)\n",
    "    \n",
    "    training_classes = label_encoder.transform(training_labels)\n",
    "    test_classes = label_encoder.transform(test_labels)  \n",
    "    return training_tfidf_vectors,test_tfidf_vectors,training_classes,test_classes,utterance_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ae131",
   "metadata": {},
   "source": [
    "##### 3.3 train the classifier and get the predictions <a class=\"anchor\" id =\"section3.3\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78667859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier and make predictions\n",
    "def train_n_pred_clf(docs_train, y_train,docs_test):\n",
    "      # Now train the Multimoda Naive Bayes classifier with the training data,\n",
    "    # and assign the trained classifier to clf\n",
    "    clf = MultinomialNB().fit(docs_train, y_train)\n",
    "    # Now let the classifier make predictions on the test data\n",
    "    y_pred = clf.predict(docs_test)\n",
    "    # why multimoda naive bayes classifier? because we are dealing with multiple labels (three categories from airlinetweets, eight from MELD), we need a multinomial classifier\n",
    "    # we may also choose another one, SVM from sklearn: svm.LinearSVC, more info https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "    return y_pred,clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d4170",
   "metadata": {},
   "source": [
    "##### 3.4 print the report <a class=\"anchor\" id =\"section3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the report\n",
    "def show_report(y_test,y_pred,target_labels):\n",
    "    report = classification_report(y_test,y_pred,digits = 7)\n",
    "    print('Report')\n",
    "    print(report)\n",
    "    print('------------------------------------------------------------')\n",
    "    # recall the labels are:\n",
    "    print('Target labels')\n",
    "    print(target_labels)\n",
    "    print('------------------------------------------------------------')\n",
    "    # print the confusion matrix, \n",
    "    # Here's an example of how to read it https://www.aboutdatablog.com/post/reading-a-confusion-matrix\n",
    "    print('Confusion matrix')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dad067",
   "metadata": {},
   "source": [
    "## 4. Run the functions <a class=\"anchor\" id =\"section4\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19079e7",
   "metadata": {},
   "source": [
    "##### 4.1 Sentiment analysis with airlinetweets <a class=\"anchor\" id =\"section4.1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19378d05",
   "metadata": {},
   "source": [
    "##### 4.2 Emotion analysis with MELD <a class=\"anchor\" id =\"section4.2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MELD\n",
    "training_instances,test_instances,training_labels,test_labels,target_labels_MELD=load_MELD_data()\n",
    "training_tfidf_vectors,test_tfidf_vectors,training_classes,test_classes,utterance_vec=data_MELD_to_nrrepr(training_instances,test_instances,training_labels,test_labels)\n",
    "y_pred,clf_MELD=train_n_pred_clf(training_tfidf_vectors, training_classes,test_tfidf_vectors)\n",
    "show_report(test_classes,y_pred,target_labels_MELD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa4bd4",
   "metadata": {},
   "source": [
    "## 5. Apply the trained classifiers  <a class=\"anchor\" id =\"section5\"></a> \n",
    "##### 5.1 Sentiment analysis trained with airlinetweets data <a class=\"anchor\" id =\"section5.1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076e6ea",
   "metadata": {},
   "source": [
    "##### 5.2 Emotion analysis trained with MELD data <a class=\"anchor\" id =\"section5.2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_emo=['Two thumbs up', \n",
    "               'I fell asleep halfway through', \n",
    "               \"We can't wait for the sequel!!\", \n",
    "               'I cannot recommend this highly enough', \n",
    "               'instant classic.', \n",
    "               'Steven Seagal was amazing.']\n",
    "gold_emo=['joy','anger','joy','joy','neutral','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-use airline_vec to transform it in the same way as the training data\n",
    "# recall: txt_senti_counts is a matrix of documents( or all sentences, each row is the vector representation of a sentence.) \n",
    "txt_emo_counts = utterance_vec.transform(txt_emo)\n",
    "#print(txt_senti_counts.shape)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# we compute tf idf values\n",
    "txt_emo_tfidf = tfidf_transformer.fit_transform(txt_emo_counts)\n",
    "# have classifier make a prediction\n",
    "y_pred_emo = clf_MELD.predict(txt_emo_tfidf)\n",
    "print(y_pred_emo)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(target_labels_MELD)\n",
    "gold_emo_classes = label_encoder.transform(gold_emo)\n",
    "\n",
    "show_report(gold_emo_classes,y_pred_emo,target_labels_MELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view which sentence gets what prediction\n",
    "for review, predicted_label in zip(txt_emo, y_pred_emo):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        label_encoder.classes_[predicted_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = []\n",
    "for review, predicted_label in zip(txt_emo, y_pred_emo):\n",
    "    content = review + \"\\t\" + label_encoder.classes_[predicted_label]\n",
    "    thing.append(content)\n",
    "\n",
    "print(thing)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8552f-4945-4bd1-97e9-366eb58af273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "filename = 'run_sent_emo.csv'\n",
    "csv_data = []   \n",
    "\n",
    "with open(filename, \"r\", encoding = 'utf-8') as infile:\n",
    "    data = infile.readlines()\n",
    "  \n",
    "    for row in data:\n",
    "        text = row.replace(\"\\n\", \"\")\n",
    "        csv_data.append(text)\n",
    "        \n",
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c8635-c72f-4f5c-9bde-634c36da51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_emo_counts = utterance_vec.transform(csv_data)\n",
    "#print(txt_senti_counts.shape)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# we compute tf idf values\n",
    "txt_emo_tfidf = tfidf_transformer.fit_transform(txt_emo_counts)\n",
    "# have classifier make a prediction\n",
    "y_pred_emo = clf_MELD.predict(txt_emo_tfidf)\n",
    "print(y_pred_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b0a6a-6d64-473f-998f-217e8ca5daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = [\"Utterance\\tEmotion\\n\"]\n",
    "for review, predicted_label in zip(csv_data, y_pred_emo):\n",
    "    content = review + \"\\t\" + label_encoder.classes_[predicted_label] + \"\\n\"\n",
    "    thing.append(content)\n",
    "    \n",
    "fulltext_tsv=\"\".join(thing)\n",
    "print(fulltext_tsv)\n",
    "\n",
    "save_filename = \"results.tsv\"\n",
    "with open(save_filename, 'w', encoding = 'utf-8') as outfile:\n",
    "    outfile.write(fulltext_tsv)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07a5cb-c952-4535-8cca-447b9f9f8227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
